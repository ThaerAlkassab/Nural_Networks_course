{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T22:15:39.729968Z",
     "iopub.status.busy": "2024-11-17T22:15:39.729535Z",
     "iopub.status.idle": "2024-11-17T22:15:39.735028Z",
     "shell.execute_reply": "2024-11-17T22:15:39.733643Z",
     "shell.execute_reply.started": "2024-11-17T22:15:39.729913Z"
    }
   },
   "source": [
    "# Static word embeddings\n",
    "\n",
    "Goal: finding meaningful low-dimensional representation of English words (or for vocabulary)\n",
    "\n",
    "- [Word2Vec - by Google AI - 2013](https://code.google.com/archive/p/word2vec/)\n",
    "  - pre-trained vectors trained on a part of the Google News dataset (about 100 billion words)\n",
    "  - model contains 300-dimensional vectors for 3 million words and phrases.  \n",
    "- [GloVe - Global Vectors for Word Representation - 2014](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "  - mapping words into a meaningful space where the distance between words is related to semantic similarity\n",
    "  - words that occur in similar contexts tend to have similar vector representations\n",
    "  - builds a word-word co-occurrence matrix where each entry represents how often a word appears in the context of another word across the corpus.\n",
    "- [FastText - by Facebook AI Research - 2016](https://arxiv.org/pdf/1612.03651)\n",
    "  - Uses subword information, unlike Word2Vec, FastText represents words as a combination of character n-grams (subword units\n",
    "  - Out-of-vocabulary robustness: by using subword information, FastText can create embeddings for words it hasn't seen during training by combining n-grams.\n",
    "\n",
    "- We can use the `gensim` module for experiments with `word2vec` embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T23:23:30.666598Z",
     "iopub.status.busy": "2024-11-17T23:23:30.666074Z",
     "iopub.status.idle": "2024-11-17T23:23:30.674562Z",
     "shell.execute_reply": "2024-11-17T23:23:30.673006Z",
     "shell.execute_reply.started": "2024-11-17T23:23:30.666545Z"
    }
   },
   "source": [
    "![Embeddings](https://miro.medium.com/v2/resize:fit:1200/1*mWerYTuy9xH4SlRY9fFg1A.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "from gensim.downloader import load\n",
    "\n",
    "model = load(\"word2vec-google-news-300\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search for similarites based on cosine similarities with the \n",
    "- `model.most_similar()` function for a key or\n",
    "- `model.similar_by_vector()` function for a vector\n",
    "\n",
    "- we can look at analogies, such as:\n",
    "$$man - woman \\approx king - queen$$\n",
    "$$queen \\approx king - man + woman$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText embeddings\n",
    "\n",
    "We can use Facebook AI's FastText embeddings via `pip install fasttext` and downloading and unzipping the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fasttext\n",
    "#!wget -qO- https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz | gunzip > cc.en.300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \n",
    " - `model.get_word_vector` for embeddings\n",
    " - `model.get_analogies` for analogies, for example we can look for:\n",
    "   - country/capital connections\n",
    "   - gender-related connections\n",
    "   - singular/plural grammar-related connections\n",
    "   - present/past tense grammar-related connections\n",
    "   - country-specific connections\n",
    "   - ordinal number-related connections (`one, two, three`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence embeddings and Classification problems\n",
    "\n",
    "- The word embeddings can be used to generate features on textual data\n",
    "  - we can either keep the original sentences or\n",
    "  - we can take an average values for the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                               text\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!wget http://rs1.sze.hu/~katihi/SMSSpamCollection\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"SMSSpamCollection\", header=None, sep=\"\\t\", names=[\"label\", \"text\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def text_to_avg_vector(sentence):\n",
    "        \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple classification model training with 300 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic embeddings - Transformer models\n",
    "\n",
    "- The previous models cannot really handle longer relations in the text and the context\n",
    "- Solution: Attention-based Transformer models with Tokenization\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "- For some popular models' vocabulary sizes are roughly:\n",
    "  - `BERT`: 30k \n",
    "  - `llama-2` - 32k\n",
    "  - `GPT3` - 50k\n",
    "\n",
    "- Embedding dimensions:\n",
    "  - `BERT` - 768\n",
    "  - `llama` - 4096\n",
    "  -  `GPT3` - 12288\n",
    "\n",
    "Important tricks in the embedding: the embeddings can change depending on\n",
    " - the position of the tokens (positional encoding)\n",
    " - and the context of the tokens, i.e the same token in the same position can change its embedding based on the context\n",
    "\n",
    "- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
    "\n",
    "![Attention](https://miro.medium.com/v2/resize:fit:1400/1*eincSo4zd1LxcOrdbK_yLw.png)\n",
    "\n",
    "- [Attention is All You Need](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "## Very Important Trick: Self-Attention!\n",
    "\n",
    "![Transformer](https://arxiv.org/html/1706.03762v7/extracted/1706.03762v7/Figures/ModalNet-21.png)\n",
    "\n",
    "![K-Q_V](https://www.lamsalashish.com.np/assets/blog/attention-is-all-you-need/image012.png)\n",
    "\n",
    "Query - Key - Value matrices:\n",
    "  - Query (Q): Represents the current token seeking information from other tokens\n",
    "  - Key (K): Represents the tokens available to provide information\n",
    "  - Value (V): Contains the actual information or content of the tokens\n",
    "\n",
    "- similarity scores are computed between the Query and each Key\n",
    "- these scores determine the attention weights, the relevance of each token\n",
    "- final output: a weighted sum of the Value vectors, guided by the attention weights\n",
    "\n",
    "![QKV](https://cdn-ilcabpl.nitrocdn.com/XTpGTaZWYQSxctfMHQPVOQKOsBspWTQi/assets/desktop/optimized/rev-a0beb67/lh3.googleusercontent.com/mOIfKc2jQ1pEZUhCMktSLcvZPBfEUAMGL-8Qp7sgE6f23S4i5tPXTN43GazvcjRywgE38t9ghmxt7nOqI-AgRLq9MVXFDqf7VuHm00aV9_6ofYqCpRMA_lTk1DOA-HFO1VCQ2uCjGOpXVEk72-nrzf7059KsCdOVfQWqxn4STCsKUDDOMpk0WlkWQt6VzQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT - Bidirectional encoder representations from transformers\n",
    "\n",
    "![BERT](https://upload.wikimedia.org/wikipedia/commons/b/b5/BERT_embeddings_01.png)\n",
    "\n",
    "\n",
    "Pre-training phase of BERT:\n",
    "\n",
    "- Masked Language Modelling: \n",
    "  - 15% of tokens would be randomly selected for masked-prediction task\n",
    "  - training objective: predict the masked token given its context.\n",
    "\n",
    "  - replaced with a [MASK] token with probability 80%,\n",
    "  - replaced with a random word token with probability 10%,\n",
    "  - not replaced with probability 10%.\n",
    "\n",
    "![MLM](https://upload.wikimedia.org/wikipedia/commons/d/d1/BERT_masked_language_modelling_task.png)\n",
    "\n",
    "- Next sentence prediction:\n",
    "  - given two spans of text, the model predicts if these two spans appeared sequentially in the training corpus\n",
    "  - outputs: either [IsNext] or [NotNext].\n",
    "\n",
    "\n",
    "\n",
    "BERT was trained on the BookCorpus (800M words) and a filtered version of English Wikipedia (2,500M words) without lists, tables, and headers.\n",
    "- Training BERT-BASE on 4 cloud TPU (16 TPU chips total) took 4 days, at an estimated cost of 500 USD. \n",
    "- Training BERT-LARGE on 16 cloud TPU (64 TPU chips total) took 4 days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different Transformer architectures\n",
    "\n",
    "- Encoder-only models work well on Classification problems where we can look at the whole text at the same time (BERT)\n",
    "- Decoder-only models work well where Text generation (chatbot) is the task (GPT), we have to predict the next word\n",
    "- Encoder-decoder models work well for Language translation tasks (T5), where\n",
    "  - we have to predict the next word in the target language\n",
    "  - but we can also look at the whole input text in the source language\n",
    "    \n",
    "![Encoder/Decoder](http://rs1.sze.hu/~katihi/encoderdecoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most of the functionalities are available via the `transformers` module both in PyTorch and TensorFlow\n",
    "\n",
    "- `pip install transformers`\n",
    "- we can simply load a tokenizer with `BertTokenizer.from_pretrained()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T03:32:47.596545Z",
     "iopub.status.busy": "2024-11-18T03:32:47.595347Z",
     "iopub.status.idle": "2024-11-18T03:32:47.604342Z",
     "shell.execute_reply": "2024-11-18T03:32:47.602887Z",
     "shell.execute_reply.started": "2024-11-18T03:32:47.596475Z"
    }
   },
   "source": [
    "#### Let's look at some tokenization \n",
    "\n",
    "- we can just apply `tokenizer()`, where can specify:\n",
    "    - truncation\n",
    "    - padding (`max_length` for example)\n",
    "    - max_length\n",
    "    - tensor format (`tf`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Fine-tuning\n",
    "\n",
    "- for classification tasks we can use the pre-trained BERT model with the `TFBertForSequenceClassification` model building\n",
    "- we should specify a smaller learning rate within the optimizer\n",
    "- use SparseCategoricalCrossentropy loss:\n",
    "  - instead of one-hot-encoding it works with multi-class classification problems where the target labels are integers representing different classes\n",
    "  - `from_logits=True`: by default the `softmax` is not yet applied, we only have the raw output values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "if 0:\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "        \n",
    "    model.compile(optimizer=Adam(learning_rate=3e-5), \n",
    "                  loss=SparseCategoricalCrossentropy(from_logits=True), \n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    history = model.fit(\n",
    "        x={\"input_ids\": train_encodings[\"input_ids\"], \"attention_mask\": train_encodings[\"attention_mask\"]},\n",
    "        y=train_labels,\n",
    "        validation_data=(\n",
    "            {\"input_ids\": test_encodings[\"input_ids\"], \"attention_mask\": test_encodings[\"attention_mask\"]},\n",
    "            test_labels\n",
    "        ),\n",
    "        epochs=3,\n",
    "        batch_size=32\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "newenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
